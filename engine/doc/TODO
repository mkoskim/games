*******************************************************************************

DONE:

- DONE: Implement shader options somehow... State object, maybe? There is now
  options[] for both shader and state. Shader specific options are applied to
  all stages where shader is used. State specific options are only for the 
  specific state.

- DONE: (step forward shader family implementation) Mesh to intermediate format,
  separate VAO buffer creation to shader family upload. It can also be local
  buffer, as it is consumed during OpenGL download.  

- DONE: Bitmap: create renderer only if it is needed (user wants to tweak
  bitmap), as it is not that common that user wants to tweak loaded/created
  bitmaps.

- REJECTED: Bitmaps... In fact, if user creates empty bitmap, we can assume
  that s/he's going to draw something on that. But it's probably just too
  advanced and don't fix any problem or affect to performance in any way.
  Added complexity for nothing, or that how it feels now.

*******************************************************************************

WORKLIST:

- ShaderToy: Web site with WebGL, has working shaders. Could be very cool
  place to go through looking for shader ideas.

- I need more freedom. What I should do is to create now a high (game) level
  interface to rendering. Then, while working with internals, that does note
  (necessarily) affect to the game functionality. It - no, they do not need
  to be perfect. I can create high level interface for specific subset of
  games.

- I should port my java applet ShootEmUp to this platform (Linux, D, OpenGL).
  There are ready-made graphics (not too high quality, though) and game logic.

- Model loading (and other nice tutorials):

    http://www.learnopengl.com/#!Model-Loading/Model

- Better integration of SkyBox (and forthcoming postprocessing effects)
  to 3D scene rendering.

- Cubemap needs all faces to be: same size, same internal format, same
  mipmap level. Ensure this in loader.

- GLSL files may need some common libraries? In any case, think about
  how to organize GLSL source files.

- Framebuffers: It is possible to share depth buffer between multiple
  framebuffers, when we are only interested in color map.

- After dividing shader implementation, there's problem with VAO as data
  packet. Somehow we need to be able to create class to keep data... It
  might require storing VAOs and such to shader itself.

- Also, one architectural problem is that now - as it is right - 3D scene
  rendering state inherits from GPU rendering state, we loose the shader
  rendering interface. Currently I am casting state shader to correct type,
  but something better should be figured out.

- I would need to collect some default setups to e.g. ext/ directory,
  so that I could work at higher levels without needing to worry about
  low level changes.

- "Active nodes" - combine Fiber and Node to ease making actively working
  nodes.

- Decals (e.g. for targeting circles)

- Examine, if BLOB could support symbolic links

- Maybe we use SDL rendering for GUI? Just to get rid of artifacts?

- Maybe own dedicated 2D shader for GUI?

- CONTINUE: Replace ulong with size_t

- Model.sheet: This is using atlas for rendering. It would be
  probably best to implement dedicated atlas sprite blitter...

- Bitmap manipulation: when loading BW bitmaps, convert them suitable
  for colorizing.

- Shader Uniform buffers? Any use?

    https://www.opengl.org/wiki/Uniform_Buffer_Object
    https://www.opengl.org/wiki/Interface_Block_(GLSL)

- Fonts & Texture loader: Fonts may need specific texture loader, for example,
  if the glyphs are zoomed considerably (e.g. text in 3D graphics).

- One thing to think about some times: we would like to activate GPU
  processing as soon as possible, to make it work parallel to CPU. But there
  are things that restrict this. For example, CPU needs to fetch all
  nodes before they can be sorted.

*******************************************************************************

MISC: Yet uncategorized "Post-It" notes...

- License - as things fall under default copyright laws without license,
  I need to think about this. Is it PD or MIT?
  
        Check: http://choosealicense.com/

- "Multi-material" meshes: It would be good to think about the
  implementation a little bit, so that it would fit to future development.
  Quickly the current thought:

        VBO + IBO   0   ... x       MeshPart 1
                    x+1 ... y       MeshPart 2
                    ...

  MeshPart is an IBO range. When rendering, you can keep both VBO
  and IBO binded, while changing material between next range.

  This is just not necessarily enough, as you may want to render parts
  in different rendering stages. As a simple example, consider a tree.
  Trees are usually constructed so that trunk and larger branches are
  solid meshes, while leafs and small branches are transparent textures.
  
  What you want, is to separate solid and transparent parts to different
  rendering stages.

- CONTINUE: Node modifiers: Add support for "multi-textured" nodes - the idea
  is to have sort of "stamping", textures over textures. We add node-specific
  material modifiers at some point (like color modifier, but at the same time,
  we could add texture modifiers, too).

- Start working with navigation. The first step is to create a network
  of nodes suitable for pacman-like games.

- Renderers in layer.d are incomplete hacks. Reorganize them, so, that
  move simplified solutions to ext/simple.d (that's the purpose of it)

- Node/Model classification. We assume, that no model (part) ever is added
  to two batches in single stage. So, what we need, is to know to what
  batch the part is added in different stages:
  
        class Model {
            VAO vao;
            struct Part {
                Batch[Stage] destination;
                material;
                (IBO) range;
            }
        }

  For example: We have two rendering pipelines, one for rendering view,
  one for rendering shadow maps. We have tree, that contains several
  materials, some of them solid, some of them transparent:
  
        tree.trunk.destination[viewrender] = viewrender.solid
        tree.trunk.destination[shadowrender] = shadowrender.solid
        
        tree.leafs.destination[viewrender] = viewrender.transparent
        tree.leafs.destination[shadowrender] = shadowrender.transparent

  Basically, we could also use some kind of tagging to locate correct
  batch. We would then also need information to which rendering stages
  the model/part is included:

        viewrender.tag = "visual";
        viewrender.solidbatch.tag = "solid";
        
        shadowrender.tag = "shadowcaster";
        shadowrender.solidbatch.tag = "solid";
        
        ...
        ...

        model.tags = ["visual", "shadowcaster"];
        model.part.tag = "solid";

- Also, it might be very good idea now to separate resource management,
  instead of keeping references in the batches. So, even thought it
  means more writing, we do the thing something like this:
  
        resources.model = shaderfamily.upload(mesh, material);
        resources.model.destination[pipeline] = pipeline.batchX;
        ...

  There is some ways to ease writing code, but anyways, the general idea
  is that references to uploaded models are found only from:

        1) nodes in scene
        2) resource lookup table for scene

  Destroying scene destroys also resources loaded for that.

- Add "persistent" flag to Batch. Even regular 3D game may have
  rendering objects that are persistent (no need for visibility checks,
  and always rendered).

- Skybox as an example of persistent objects in batches. Are they? Currently
  I am using own dedicated shader for skybox, so, they are not objects in
  batches, but own dedicated "batch".

-------------------------------------------------------------------------------

HOTTISH TOPICS:

-------------------------------------------------------------------------------

FIX/INVESTIGATE: Crashes, flaws etc.

- IMPORTANT: Querying shader uniform and attribute locations will not work
  correctly if the shader is not activated (the functions will return the
  locations of the active shader). So, during uploading, we should ensure that the
  shader (or at least its family) is active. Another way to do this is to use
  fixed locations, but it sounds very laborous and inflexible solution.

- FIX: glGenerateMipmaps() seems not to work when using OpenGL 3.3 - with GL2.1
  it works.

- FIX: Font rendering fails randomly:

    http://stackoverflow.com/questions/26956658/sdl-ttf-rendertext-blended-fails-randomly

- FIX: On 32-bit Ubuntu, SDL FillRect segfaults (code that cuts textures from
  sprite sheet).

- Shader compiler (see gpucompiler.d) can only report error location for
  noveau driver. Needs (1) driver detection, and (2) custom reporting for
  each driver.

- With normalized integer vectors (see gputypes.d), we use scaling, but
  in reality (for 8-bit) -1 -> -128, +1 -> +127.

- TBN calculation in current shader has its flaws (not normalized nor
  orthogonal in fragment shader), but it might be so that it has no
  significant effect to rendering result

- Current Bone implementation has a transformation cache. It is unclear to me,
  how it works with games loading levels (that is, does it cause memory leak).

- wolfish (and cylinderium) generally run at least as fast as pacman, although
  they use much more complicated CPU and GLSL code. I need to investigate 2D
  performance, as because of GUI elements it will matter to 3D games, too.

- Study: Do we find any use for uniform buffers? See:

        https://www.opengl.org/wiki/Uniform_Buffer_Object

-------------------------------------------------------------------------------

DEBUG FEATURES, PROGRAMMING HELPERS: Features for helping to debug and develop
things:

- "Debug channel" to rendering batches: adding a debug mesh to an object
  will be sent there. Geometry shaders could be excellent for this purpose?
  To create debugging primitives (lines for normals, ...) automatically?
  
- It would be nice, if we could direct nodes / entire groups to
  "Wireframe shader"

- Found enforce from D documentation. It is like my old ERRORIF, a bit more
  advanced, though, because using template to allow it to return the value if
  it was OK. Go through code to find good places for it! Also, I implemented
  ERRORIF using same principle - this is good to give simple error message,
  instead of getting Exception dumping call tree.

- "CPU rendering": Rendering objects without uploading them to shader
  (as VAOs, VBOs), like simulated immediate mode. This might be good for
  example for debugging (rendering normally invisible things). Low priority,
  thought, at least initially.


-------------------------------------------------------------------------------

"USER-FRIENDLINESS": Some planned efforts to make code more "user-friendly":
more portable (32-bit, 64-bit, Linux, Windows), easier to set up, and so on.

- SDL FillRect & Font rendering segfaults on 32-bit machine

- It would be nice to add D doc comments, at least test it. Finally, it would
  be nice if I could make it to compile with dub, and use dub registry.

-------------------------------------------------------------------------------

USER INPUT:

- Keyboard emulated game controller

- Make game controller optional, at least for games that does not need them
  that much (i.e. have also keyboard control)

-------------------------------------------------------------------------------

GENERAL:

- Static linking: examine possibilities to link needed libraries
  (SDL?) statically to remove system side dependencies.

- VBO updating - or, in fact, "CPU-side" VBOs, mainly for instanced draws
  like particle engines. See e.g.
  
  * https://www.opengl.org/wiki/Buffer_Object_Streaming

  * http://www.opengl-tutorial.org/intermediate-tutorials/billboards-particles/particles-instancing/

- Layer <-> geom needs reference point... This is/was intented to help creating
  2D HUD & games. It may be so that 2D games get their own Scene class some
  day? And anyways, geom module has had not much to do with 3D graphics ever
  since implementing Wavefront loader.

- Maybe some offline tools for processing images, meshes and materials?

- Do some debugging aids: Seeing matrices etc.

-------------------------------------------------------------------------------

LOADERS, FILE FORMATS: We started to use Assimp to load model files. Some
design, sketching and planning is needed, as well as tuning the render
architecture more suitable for models loaded from files.

- Currently there seems to be some sort of a bug with LuaD and DMD 2.067+
  compilers causing it to segfault at exit. I might want to postpone the
  development until the problem is found & fixed.

- CONSIDER SERIOUSLY adding scripting support for asset loading! The idea
  here is, that e.g. lua scripts to load specific things are stored with
  the models, and thus they are stored to BLOB, too - no need for complex
  building mechanism to include correct converters to compiled program.

- Blender file (archive) loading

- COLLADA file loading. This project has some possibly interesting
  Collada files to try out:

    https://bitbucket.org/EricPoggel/yage/src/c12a0715d0a7?at=default

- BLOB "attachments": Larger games probably like to have separate resource
  files to ease updating. Without breaking existing behaviour, this could
  be done so that it is possible to include external archives/directories
  to look up for files.
  
- BLOB: Game saving: At some point, we need to add file writing to the engine.

- Wavefront loader: change it to use ASSIMP

- Wavefront loader: smoothed normals

- Wavefront loader & mesh-material pairs: Wavefront file format has
  groups (s[moothing group], g[group), which could be used to load
  mesh-material pairs, so that they can use same VBOs. Between each
  batch, we change IBO and material - nah, no need, we can give
  range to drawing command, and thus keep the same IBO.

- Some simple file format to create scenes (placing objects, creating
  object hierarchies, ...) could be useful. POVRay may be too complex.
  Something that can be used together with .obj & .mtl files... Of
  course, we need support to other mesh data formats, too, but .obj is
  quite common & useful anyways.

- Winding: In case that winding is incorrect when loading a model, we
  could have methods in Mesh class to try to fix it. At least, this
  would be better than changing winding parameter (CW / CCW) when
  rendering.

-------------------------------------------------------------------------------

FIBERS, EVENT PROCESSING:

- It would be good to be able to send events to be processed by a Fiber.
  If we have player Fiber, we might want to get the input events there.
  Think & design how to do this.

- It might be good, that FiberQueue callbacks return boolean value
  if they want to continue processing, or if they want them removed. Other
  way to do that would be adding the calling queue to parameters, so that
  callback can remove itself if wanted

- It would be good to implement (frame) timers to Fiber queue.

- simple.gameloop may need a function to determine loop breaking? That
  could simplify certain things...

- Check vibe.d for ideas for game actors (fibers)

- It would be good to create active nodes (nodes that are both nodes
  and fibers).

-------------------------------------------------------------------------------

SHADERS: Shaders are so central part that they reserve their own section
here. Shader is connected to at least objects, materials and scenes, so these
issues have their relations to other sections in this TODO document.

- ONGOING: 3D shader, 2D GUI shader, and postprocessing effect shaders, they
  all have very different interface. Because of that, I probably need to drop
  Shader implementation even lower level, and create intermediate structures
  between render, canvas and such --> Because of this, I might consider taking
  glamour there.

- Shaders: Raymarching? What is that? Realtime raytracing is the future. I
  should investigate it very closely!

- Shaders: Interior mapping, quite cool:

    http://interiormapping.oogst3d.net/

- I might need to go to version 130 with shaders. For creating compatible shader
  banks, it is mandatory to ensure that attributes are at the same location.
  Otherwise, loaded VBOs (via VAO) do not work: not a problem at the moment, as
  attributes are bind by hand.

- Half-done: Improve introspection: dumping uniforms etc at compilation module, to
  help improving & debugging shaders.

- Design "multi-shader" shader: A shader that has several compiled shader
  programs, that can share the same instances.

- Some sort of batch processing with shaders? See:

    * http://www.gamedev.net/page/resources/_/technical/opengl/opengl-batch-rendering-r3900

- It is possible to compile GLSL library code to shader program, and then
  link multiple of these programs together with specific main(). Investigate
  this:
  
      http://stackoverflow.com/questions/9168252/attaching-multiple-shaders-of-the-same-type-in-a-single-opengl-program

- Think Shader subsystem. We would like to have specialized shaders for
  particle engines, not using the regular mesh-material data structures.

- Shader family: It could be time to concentrate on shader management
  before going forward:

  * Shader family: to upload stuff to GPU w/o creating actual shader

    - Problem: for binding VBOs and VAOs, the shader program needs
      to be active...

  * Shader compilation: for error reporting, would be good to create
    "precompiled" libraries. I have a feeling that I can never make
    error reporting reliable (w/ current OpenGL implementation) for
    "preprocessed" sources, so it would be best to compile GLSL files
    just as the programmer made them.

  * Move default shaders to ext/ directory... Would be better to locate
    them to stock/ - or create a new directory for defaults...

- Move default shaders under ext/ directory (?)

-------------------------------------------------------------------------------

OPTIMIZATIONS: While optimizations are not that important, here is some list
of optimizing features I might consider for implementing. For that purpose,
keep these in mind so that architecture does not prevent implementing these:

- Organize triangles to strips, study primitive restart:

    * glEnable(GL_PRIMITIVE_RESTART)​
    * glPrimitiveRestartIndex(65535)​
    * https://www.opengl.org/wiki/Vertex_Rendering#Primitive_Restart

- Sorting drawing so that the same vertice is preferably used in consecutive
  calls, to improve GPU cache performance.

- Think if it is possible automatically pad things:

    "Padding is added to make the vertex structure a multiple of 32 bytes
     since some GPUs prefer it that way, such as ATI/AMD."

    https://www.opengl.org/wiki/Client-Side_Vertex_Arrays

-------------------------------------------------------------------------------

SCENES, LAYERS:

- I'm still not entirely satisfied with the current system. It is currently
  possible to use batch groups directly as scene graphs, which is basically
  good for games like pacman. But:
  
  * We should have node sorting (front to back, ...): It is possible
    to do at draw function, thought.
  
  * We could remove viewing information from scene node, and add it
    to batch node.

- How about making unbuffered batch? Currently, nodes are buffered to
  batch, but we could send them directly to shader: basically, if
  first batch in batch group is unordered, it could be unbuffered, too.
  There is - of course - problem to determine when render state block
  is applied...

- Store viewing information to batch node, not to scene node. Same node
  may be retrieved multiple times from different angles.

- Probably it is better to classify objects at Layer/Scene level already...
  Keeping objects in one single list helps lookups, but (1) I assume that
  they are more rare operations than rendering calls, and (2) that can
  be simulated anyways (lookup looks from all lists).
  
  * Look at Cylinderium: it has 2 interleaved scenes, one for mother ship,
    one for star field. We might consider generalizing something similar?

- Because of this, it might make some sense to bind lights, visibility
  checks and such to camera. The same information could then be used
  by all scene layers. Need thinking...

- Skybox: Cylinderium could use this, in addition to star field. Skybox
  does not replace star field - star field is three dimensional, skybox
  is basically just two dimensional background image.

- Occlusion queries?

    * https://www.opengl.org/wiki/Query_Object#Occlusion_queries

-------------------------------------------------------------------------------

3D OBJECTS:

- Reconsider rotation, it's very unintuitive at the moment. Currently you
  need to make parent hierarchies to keep rotations in order...

- Currently, we have Grip class, that generates transform matrices from
  position, rotation and scaling. It is possible, that we need also
  other ways to generate transform matrices - so, be prepared for that.

    * We might want to get Grips from game logic/physics layer (e.g.
      portal, or navmesh)
  
- Scaling is not entirely working, mainly because it would need updating of
  bounding sphere. Non-uniform scaling is anyways pretty hard to make
  to work correctly (affects to normals etc).

- Points & lines: (1) in meshes, separate points, lines and triangles
  (different IBOs for each; combine somehow with material groups). Think
  a system to tell point size (glPointSize), and finally, we might have
  particle engine...
  
  * In fact: Meshes are meshes. Lines and points are rendered by different
    shaders, or at least, they form different kind of objects, not meshes.
    
- Multiple mesh-material pairs in same object: Maybe with vertex groups?
  Adding IBO-Material pairs to VAOs? No need for separate IBOs, just start
  index for each batch.

- Ways to extend Mesh class. Some way to attach more vertex data to
  Mesh so that Shader understands it, for example, skeleton.

- LOD, Level of Detail: like textures have mipmaps, 3D objects could also have
  representations in various details. Including materials. And shaders? No
  need for high-quality shading for low-detail objects.

- Billboards and imposters

- Half-done: Instanced rendering: Layers with single mesh type for OpenGL
  instanced rendering? Probably no, it is probably better idea to have "instanced
  instance", instance with single shape and multiple positions. But well...
  Maybe a particle layer is not a bad idea? Need thinking...
  
- Where instanced rendering would perform best? If there is multiples of same
  mesh near each other, then we could take the advantage of both worlds:
  having efficient CPU-side prunings, AND instanced rendering. For example,
  branches and leaves in a tree.

- Investigate using of GL_ARB_vertex_attrib_binding functions to bind
  buffers and attributes, instead of glVertexAttribPointer.

-------------------------------------------------------------------------------

PARTICLE & SPRITE ENGINE: It might be nice to have specialized
classes to blit lots of things into 2D/3D space.

- Found OpenGL Array Textures. Can these be used instead of atlases? For
  sprite sheet blitters?

    https://www.opengl.org/wiki/Array_Texture

- Particle engine can (most probably) have only 1 texture. So, the solution
  is to create sprite sheets?
  
- Anyways, there are probably at least three different sprite engines:

  1) Lowest performance engine works like current TextBox: We could add
     depth things there, so that you could draw text background *after*
     printing the text. Or can we? It will be tricky...

  2) Highest performance engine is "regular" particle engine.

- We might want to separate static and dynamic sprite engines: backgrounds,
  starfields and such would be rendered by static engine, moving sprites
  by dynamic engine.

-------------------------------------------------------------------------------

ANIMATION:

- Animation

- EasingCurve: http://qt-project.org/doc/qt-4.8/qeasingcurve.html

- Blender: http://wiki.blender.org/index.php/Doc:2.6/Manual/Animation

- One tough question: How to update bounding volumes on animated meshes?
  Use coarse approximation?

- Easy curve generator to tools... Some curves are easier to generate
  numerically with simulator...

-------------------------------------------------------------------------------

LIGHTS, MATERIALS: Lights and materials are fragment shader inputs. It would be
good that these are generic (but simple enough) to support different kinds of
shaders.

- Design light management... Shaders will always possess limitations to how
  many lights they can handle per object

- Shadows

- Study other shading models

- Half-done: Design some sort of mechanism for some uniform material parameters,
  so that you can change shading model and still have somewhat similar results

- It would be nice to have some kind of "database" system for materials,
  combining e.g. color & normal maps and roughness together, so that you
  could load materials with fewer lines in the code. Similarly, there
  could be data files for other assets like sprite sheets (describing the
  dimensions of the sheet) for loading them with less efforts.

- It might be wise to move normal maps from material to "mesh": it is more
  like added geometry than a material parameter... Although it can be
  that, too...

- Environment mapping (reflections):

    http://learnopengl.com/#!Advanced-OpenGL/Cubemaps

-------------------------------------------------------------------------------

CAMERA:

- Viewport settings to camera... In fact, create sort of RenderTarget
  class.

- View, ViewProxy: I havent used View proxy ever. There is a need to
  check if it has any use at all, or if we could simplify things a bit.

-------------------------------------------------------------------------------

TEXTURES:

- Texture update (updating Texture pixel information from CPU side)

- Texture download (downloading e.g. rendered Texture to CPU side)

- Rename Texture.Loader to Texture.Sampler?

    https://www.opengl.org/wiki/Sampler_Object

-------------------------------------------------------------------------------

BITMAPS:

- CPU-side bitmap operations that could be useful: (1) scaling, (2) adding/
  removing alpha layer. Maybe (3) "loadtime" "stamping" (blitting bitmaps over
  bitmaps): we most probably need the feature in shader, but we could also
  support combining bitmaps at loadtime.

- Load-time scaling of bitmaps (some kind of image manipulation
  library)

- Some sort of "tile toolkit" to create tiles?

- Also, some sort of normal map toolkit to create normal maps

- Atlas creation

- Bitmap.splitSheet: Scaling splitted sprites does not work.

-------------------------------------------------------------------------------

GUI, HUD: GUI sketching/planning/designing is ongoing. Sure, the focus is on
game rendering engine (render/ directory structure), but also sure, GUI/HUD
is pretty damn important to play a game.

-------------------------------------------------------------------------------

FONTS:

- For some reason, rendered text at plain alpha channel does not work.
  Investigate.

- Half-done: Bitmap sheet loading need to be generalized, needed for both text
  and (at least 2D) game objects 

- At some point, we need to modify text to support UTF-8 (for moe3d)

- Investigate possible alternative solutions for rendering (HUD) text. Maybe
  rendering at CPU, and using subtextures?

- Also, we could use GPU to scale letters, no need to create font for all
  sizes. Check the result.

- Also, current system is made for HUD (screen coordinates), design a way to
  use it for 3D objects

-------------------------------------------------------------------------------

SOUNDS:

- Think a little bit about them...

-------------------------------------------------------------------------------

ARCHITECTURE:

- Shaders and layers... VAOs, VBOs and such, they are sort of bind to shader.
  At some point, we might need to ensure that shader can't be switched after
  meshes in layer are bind to certain shader. Or something like that.

-------------------------------------------------------------------------------

LOW PRIORITY:

- Study Manhattan distance to be used as quick way to filter nearby elements

